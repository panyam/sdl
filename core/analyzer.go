package core

import (
	"fmt"
	"math"
	"strings"
	"testing"
)

// Assumes access to metrics.go functions (Availability, MeanLatency, etc.)

// MetricType defines the specific metric to check.
type MetricType int

const (
	AvailabilityMetric MetricType = iota
	MeanLatencyMetric
	P50LatencyMetric
	P99LatencyMetric
	P999LatencyMetric
	// TODO: Could add P0, P100, specific bucket checks later if needed
)

// OperatorType defines the comparison operation.
type OperatorType int

const (
	LT  OperatorType = iota // Less Than (<)
	LTE                     // Less Than or Equal To (<=)
	GT                      // Greater Than (>)
	GTE                     // Greater Than or Equal To (>=)
	EQ                      // Equal To (==) (Use with caution for floats)
	NEQ                     // Not Equal To (!=)
)

// Expectation defines a single check to perform on the analysis results.
type Expectation struct {
	Metric    MetricType
	Operator  OperatorType
	Threshold float64 // The value to compare against
}

// ExpectationResult holds the outcome of a single expectation check.
type ExpectationResult struct {
	Expectation Expectation // The original expectation
	ActualValue float64     // The calculated metric value
	Passed      bool        // Did the check pass?
}

type AnalysisResultInterface interface {
	GetName() string
	IsPerformed() bool
	GetAllPassed() bool
	GetExpectationChecks() []ExpectationResult
	GetMetrics() map[MetricType]float64 // Returns calculated metrics

	// Method to access underlying outcomes if needed (returns any)
	GetCoreOutcomesAny() any

	// We can add convenience methods for common metrics if desired
	Availability() (float64, bool) // Returns value, true if metric exists
	MeanLatency() (Duration, bool)
	P50Latency() (Duration, bool)
	P99Latency() (Duration, bool)
	P999Latency() (Duration, bool)

	// Keep logging/assertion helpers here, but maybe attach to the concrete type later?
	// Or make them standalone functions taking the interface? Let's keep them for now.
	LogResults(t *testing.T)
	Assert(t *testing.T)
	AssertFailure(t *testing.T)
}

// AnalysisResult holds the complete results of running an analysis.
type AnalysisResult[V Metricable] struct {
	Name              string                 // Name of the analysis run
	AnalysisPerformed bool                   // False if simulation returned nil/empty outcomes
	Outcomes          *Outcomes[V]           // The raw outcomes distribution (optional, could be large)
	Metrics           map[MetricType]float64 // Calculated metrics
	ExpectationChecks []ExpectationResult    // Results of each expectation check
	AllPassed         bool                   // True if all expectations passed (or no expectations were given)
}

// --- Implementation of AnalysisResultInterface for AnalysisResult[V] ---

func (ar *AnalysisResult[V]) GetName() string                           { return ar.Name }
func (ar *AnalysisResult[V]) IsPerformed() bool                         { return ar.AnalysisPerformed }
func (ar *AnalysisResult[V]) GetAllPassed() bool                        { return ar.AllPassed }
func (ar *AnalysisResult[V]) GetExpectationChecks() []ExpectationResult { return ar.ExpectationChecks }
func (ar *AnalysisResult[V]) GetMetrics() map[MetricType]float64        { return ar.Metrics }
func (ar *AnalysisResult[V]) GetCoreOutcomesAny() any                   { return ar.Outcomes }

func (ar *AnalysisResult[V]) Availability() (float64, bool) {
	v, ok := ar.Metrics[AvailabilityMetric]
	return v, ok
}
func (ar *AnalysisResult[V]) MeanLatency() (Duration, bool) {
	v, ok := ar.Metrics[MeanLatencyMetric]
	return v, ok // Duration is float64
}
func (ar *AnalysisResult[V]) P50Latency() (Duration, bool) {
	v, ok := ar.Metrics[P50LatencyMetric]
	return v, ok
}
func (ar *AnalysisResult[V]) P99Latency() (Duration, bool) {
	v, ok := ar.Metrics[P99LatencyMetric]
	return v, ok
}
func (ar *AnalysisResult[V]) P999Latency() (Duration, bool) {
	v, ok := ar.Metrics[P999LatencyMetric]
	return v, ok
}

// LogResults formats and logs the AnalysisResult to a *testing.T object.
// It mimics the output previously generated by Analyze directly.
func (ar *AnalysisResult[V]) LogResults(t *testing.T) {
	t.Helper()
	if !ar.AnalysisPerformed {
		if ar.Outcomes == nil {
			t.Logf("[%s] Analysis skipped: Simulation returned nil outcomes", ar.Name)
		} else {
			t.Logf("[%s] Analysis skipped: Simulation returned empty outcomes (0 buckets)", ar.Name)
		}
		return
	}

	// Log calculated metrics
	var metricsParts []string
	// Ensure consistent order for logging
	metricsOrder := []MetricType{AvailabilityMetric, MeanLatencyMetric, P50LatencyMetric, P99LatencyMetric, P999LatencyMetric}
	for _, mt := range metricsOrder {
		if val, ok := ar.Metrics[mt]; ok {
			format := "%.6f"
			suffix := "s"
			if mt == AvailabilityMetric {
				format = "%.6f"
				suffix = ""
			}
			metricsParts = append(metricsParts, fmt.Sprintf("%s="+format+"%s", metricTypeToString(mt), val, suffix))
		}
	}
	bucketCount := 0
	if ar.Outcomes != nil {
		bucketCount = ar.Outcomes.Len()
	}
	t.Logf("[%s] Results: %s (Buckets: %d)",
		ar.Name, strings.Join(metricsParts, ", "), bucketCount)

	// Log expectation checks
	for i, check := range ar.ExpectationChecks {
		metricName := metricTypeToString(check.Expectation.Metric)
		opStr := operatorTypeToString(check.Expectation.Operator)
		status := "PASS"
		if !check.Passed {
			status = "FAIL"
		}
		t.Logf("[%s] EXPECT #%d: %s %s %.6f -> ACTUAL: %.6f -> %s",
			ar.Name, i+1, metricName, opStr, check.Expectation.Threshold, check.ActualValue, status)
	}
}

// Assert expects the analysis to have passed all checks and fails the test if not.
func (ar *AnalysisResult[V]) Assert(t *testing.T) {
	t.Helper()
	ar.LogResults(t) // Log the details first
	if !ar.AnalysisPerformed && len(ar.ExpectationChecks) > 0 {
		t.Errorf("[%s] Failed: Expected outcomes but simulation returned nil or empty", ar.Name)
	} else if !ar.AllPassed {
		t.Errorf("[%s] Failed: One or more expectations were not met.", ar.Name)
	}
}

// AssertFailure expects the analysis to have failed at least one check and fails the test if not.
func (ar *AnalysisResult[V]) AssertFailure(t *testing.T) {
	t.Helper()
	ar.LogResults(t) // Log the details first
	if !ar.AnalysisPerformed && len(ar.ExpectationChecks) > 0 {
		// This is arguably a failure if expectations existed but couldn't run
		t.Errorf("[%s] Failed: Expected outcomes but simulation returned nil or empty", ar.Name)
	} else if ar.AllPassed {
		t.Errorf("[%s] Failed: Expected one or more expectations to fail, but all passed.", ar.Name)
	}
}

// --- Helper functions for creating Expectations (Fluent API style) ---

func ExpectAvailability(op OperatorType, threshold float64) Expectation {
	return Expectation{Metric: AvailabilityMetric, Operator: op, Threshold: threshold}
}

func ExpectMeanLatency(op OperatorType, threshold float64) Expectation {
	return Expectation{Metric: MeanLatencyMetric, Operator: op, Threshold: threshold}
}

func ExpectP50(op OperatorType, threshold float64) Expectation {
	return Expectation{Metric: P50LatencyMetric, Operator: op, Threshold: threshold}
}

func ExpectP99(op OperatorType, threshold float64) Expectation {
	return Expectation{Metric: P99LatencyMetric, Operator: op, Threshold: threshold}
}

func ExpectP999(op OperatorType, threshold float64) Expectation {
	return Expectation{Metric: P999LatencyMetric, Operator: op, Threshold: threshold}
}

// sdl/core/analyzer.go (Continued)

// Analyze executes a simulation function, calculates standard metrics, logs them,
// and checks against provided expectations, failing the test if expectations are not met.
// It requires the outcome type V to implement the Metricable interface.
func Analyze[V Metricable](name string, simulationFunc func() *Outcomes[V], expectations ...Expectation) AnalysisResult[V] {

	result := AnalysisResult[V]{
		Name:              name,
		AnalysisPerformed: false,
		Metrics:           make(map[MetricType]float64),
		ExpectationChecks: make([]ExpectationResult, 0, len(expectations)),
		AllPassed:         true, // Assume true until proven otherwise
	}

	// --- Execute Simulation ---
	outcomes := simulationFunc()
	result.Outcomes = outcomes // Store the raw outcomes

	// Handle nil or empty outcomes gracefully
	if outcomes == nil || outcomes.Len() == 0 {
		// AnalysisPerformed remains false
		result.AllPassed = len(expectations) == 0 // Pass if no expectations, fail otherwise
		return result
	}

	// --- Analysis Performed ---
	result.AnalysisPerformed = true

	// --- Calculate Standard Metrics ---
	avail := Availability(outcomes)
	meanLat := MeanLatency(outcomes)
	p50 := PercentileLatency(outcomes, 0.50)
	p99 := PercentileLatency(outcomes, 0.99)
	p999 := PercentileLatency(outcomes, 0.999)

	result.Metrics[AvailabilityMetric] = avail
	result.Metrics[MeanLatencyMetric] = meanLat
	result.Metrics[P50LatencyMetric] = p50
	result.Metrics[P99LatencyMetric] = p99
	result.Metrics[P999LatencyMetric] = p999

	// --- Evaluate Expectations ---
	for _, exp := range expectations {
		var check ExpectationResult
		check.Expectation = exp

		actualValue, metricExists := result.Metrics[exp.Metric]
		if !metricExists {
			// Cannot evaluate this expectation
			check.ActualValue = math.NaN() // Or some indicator
			check.Passed = false           // Treat as failed if metric doesn't exist? Or skip? Let's fail it.
		} else {
			check.ActualValue = actualValue
			check.Passed = checkCondition(actualValue, exp.Operator, exp.Threshold)
		}

		result.ExpectationChecks = append(result.ExpectationChecks, check)
		if !check.Passed {
			result.AllPassed = false // If any check fails, the overall result is failure
		}
	}

	return result
}

// --- Private helper functions ---

func checkCondition(actual float64, op OperatorType, threshold float64) bool {
	// Use a small tolerance for float comparisons in EQ/NEQ
	const tolerance = 1e-9
	switch op {
	case LT:
		return actual < threshold
	case LTE:
		return actual <= threshold
	case GT:
		return actual > threshold
	case GTE:
		return actual >= threshold
	case EQ:
		return math.Abs(actual-threshold) < tolerance
	case NEQ:
		return math.Abs(actual-threshold) >= tolerance
	default:
		return false // Should not happen
	}
}

func metricTypeToString(m MetricType) string {
	switch m {
	case AvailabilityMetric:
		return "Availability"
	case MeanLatencyMetric:
		return "MeanLatency"
	case P50LatencyMetric:
		return "P50"
	case P99LatencyMetric:
		return "P99"
	case P999LatencyMetric:
		return "P999"
	default:
		return "UnknownMetric"
	}
}

func operatorTypeToString(op OperatorType) string {
	switch op {
	case LT:
		return "<"
	case LTE:
		return "<="
	case GT:
		return ">"
	case GTE:
		return ">="
	case EQ:
		return "=="
	case NEQ:
		return "!="
	default:
		return "?"
	}
}
